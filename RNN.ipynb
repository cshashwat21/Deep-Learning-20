{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "Ok9-M1oLCEMQ",
    "outputId": "b65f149f-259e-4967-a2c9-e3fad9b3eb9f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-2829c8c3-d5d6-46d4-9784-bdd6ebbc66e4\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-2829c8c3-d5d6-46d4-9784-bdd6ebbc66e4\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving omni.txt to omni.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'omni.txt': b'\"They\\'re made out of meat.\"\\r\\n\\r\\n     \"Meat?\"\\r\\n\\r\\n     \"Meat. They\\'re made out of meat.\"\\r\\n\\r\\n     \"Meat?\"\\r\\n\\r\\n     \"There\\'s no doubt about it. We picked up several from different parts of the planet, took them aboard our recon vessels, and probed them all the way through. They\\'re completely meat.\"\\r\\n\\r\\n     \"That\\'s impossible. What about the radio signals? The messages to the stars?\"\\r\\n\\r\\n     \"They use the radio waves to talk, but the signals don\\'t come from them. The signals come from machines.\"\\r\\n\\r\\n     \"So who made the machines? That\\'s who we want to contact.\"\\r\\n\\r\\n     \"They made the machines. That\\'s what I\\'m trying to tell you. Meat made the machines.\"\\r\\n\\r\\n     \"That\\'s ridiculous. How can meat make a machine? You\\'re asking me to believe in sentient meat.\"\\r\\n\\r\\n     \"I\\'m not asking you, I\\'m telling you. These creatures are the only sentient race in that sector and they\\'re made out of meat.\"photomaxmix\\r\\n\\r\\n     \"Maybe they\\'re like the orfolei. You know, a carbon-based intelligence that goes through a meat stage.\"\\r\\n\\r\\n     \"Nope. They\\'re born meat and they die meat. We studied them for several of their life spans, which didn\\'t take long. Do you have any idea what\\'s the life span of meat?\"\\r\\n\\r\\n     \"Spare me. Okay, maybe they\\'re only part meat. You know, like the weddilei. A meat head with an electron plasma brain inside.\"\\r\\n\\r\\n     \"Nope. We thought of that, since they do have meat heads, like the weddilei. But I told you, we probed them. They\\'re meat all the way through.\"\\r\\n\\r\\n     \"No brain?\"\\r\\n\\r\\n     \"Oh, there\\'s a brain all right. It\\'s just that the brain is made out of meat! That\\'s what I\\'ve been trying to tell you.\"\\r\\n\"So ... what does the thinking?\"\\r\\n\\r\\n     \"You\\'re not understanding, are you? You\\'re refusing to deal with what I\\'m telling you. The brain does the thinking. The meat.\"\\r\\n\\r\\n     \"Thinking meat! You\\'re asking me to believe in thinking meat!\"\\r\\n\\r\\n     \"Yes, thinking meat! Conscious meat! Loving meat. Dreaming meat. The meat is the whole deal! Are you beginning to get the picture or do I have to start all over?\"\\r\\n\\r\\n     \"Omigod. You\\'re serious then. They\\'re made out of meat.\"\\r\\n\\r\\n     \"Thank you. Finally. Yes. They are indeed made out of meat. And they\\'ve been trying to get in touch with us for almost a hundred of their years.\"\\r\\n\\r\\n     \"Omigod. So what does this meat have in mind?\"\\r\\n\\r\\n     \"First it wants to talk to us. Then I imagine it wants to explore the Universe, contact other sentiences, swap ideas and information. The usual.\"\\r\\n\\r\\n     \"We\\'re supposed to talk to meat.\"\\r\\n\\r\\n     \"That\\'s the idea. That\\'s the message they\\'re sending out by radio. \\'Hello. Anyone out there. Anybody home.\\' That sort of thing.\"\\r\\n\\r\\n     \"They actually do talk, then. They use words, ideas, concepts?\"\\r\\n\\r\\n     \"Oh, yes. Except they do it with meat.\"\\r\\n\\r\\n     \"I thought you just told me they used radio.\"\\r\\n\\r\\n     \"They do, but what do you think is on the radio? Meat sounds. You know how when you slap or flap meat, it makes a noise? They talk by flapping their meat at each other. They can even sing by squirting air through their meat.\"\\r\\n\\r\\n     \"Omigod. Singing meat. This is altogether too much. So what do you advise?\"\\r\\n\\r\\n     \"Officially or unofficially?\"\\r\\n\"Both.\"\\r\\n\\r\\n     \"Officially, we are required to contact, welcome and log in any and all sentient races or multibeings in this quadrant of the Universe, without prejudice, fear or favor. Unofficially, I advise that we erase the records and forget the whole thing.\"\\r\\n\\r\\n     \"I was hoping you would say that.\"\\r\\n\\r\\n     \"It seems harsh, but there is a limit. Do we really want to make contact with meat?\"\\r\\n\\r\\n     \"I agree one hundred percent. What\\'s there to say? \\'Hello, meat. How\\'s it going?\\' But will this work? How many planets are we dealing with here?\"\\r\\n\\r\\n     \"Just one. They can travel to other planets in special meat containers, but they can\\'t live on them. And being meat, they can only travel through C space. Which limits them to the speed of light and makes the possibility of their ever making contact pretty slim. Infinitesimal, in fact.\"\\r\\n\\r\\n     \"So we just pretend there\\'s no one home in the Universe.\"\\r\\n\\r\\n     \"That\\'s it.\"\\r\\n\\r\\n     \"Cruel. But you said it yourself, who wants to meet meat? And the ones who have been aboard our vessels, the ones you probed? You\\'re sure they won\\'t remember?\"\\r\\n\\r\\n     \"They\\'ll be considered crackpots if they do. We went into their heads and smoothed out their meat so that we\\'re just a dream to them.\"\\r\\n\\r\\n     \"A dream to meat! How strangely appropriate, that we should be meat\\'s dream.\"\\r\\n\\r\\n     \"And we marked the entire sector unoccupied.\"\\r\\n\\r\\n     \"Good. Agreed, officially and unofficially. Case closed. Any others? Anyone interesting on that side of the galaxy?\"\\r\\n\\r\\n     \"Yes, a rather shy but sweet hydrogen core cluster intelligence in a class nine star in G445 zone. Was in contact two galactic rotations ago, wants to be friendly again.\"\\r\\n\\r\\n     \"They always come around.\"\\r\\n\\r\\n     \"And why not? Imagine how unbearably, how unutterably cold the Universe would be if one were all alone ...\"'}"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "A RNN implementation example using TensorFlow..\n",
    "Next word prediction after n_input words learned from text file.\n",
    "A story is automatically generated if the predicted word is fed back as input.\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqog_leoCkIw"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vbjYT9hnC-Pf",
    "outputId": "3563f5a0-65e9-4de6-e3db-ce986c009306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data...\n"
     ]
    }
   ],
   "source": [
    "# Text file containing words for training\n",
    "training_file = 'omni.txt'\n",
    "\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [word for i in range(len(content)) for word in content[i].split()]\n",
    "    content = np.array(content)\n",
    "    return content\n",
    "\n",
    "training_data = read_data(training_file)\n",
    "print(\"Loaded training data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgUZYpopdLr-"
   },
   "outputs": [],
   "source": [
    "logs_path = '/tmp/tensorflow/rnn_words'\n",
    "writer = tf.summary.FileWriter(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CODDKaIoqgkw",
    "outputId": "a4961c3c-f4b9-4c7c-98c2-d5ccde42b19e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\"They\\'re', 'made', 'out', 'of', 'meat.\"', '\"Meat?\"', '\"Meat.',\n",
       "       \"They're\", 'made', 'out', 'of', 'meat.\"', '\"Meat?\"', '\"There\\'s',\n",
       "       'no', 'doubt', 'about', 'it.', 'We', 'picked', 'up', 'several',\n",
       "       'from', 'different', 'parts', 'of', 'the', 'planet,', 'took',\n",
       "       'them', 'aboard', 'our', 'recon', 'vessels,', 'and', 'probed',\n",
       "       'them', 'all', 'the', 'way', 'through.', \"They're\", 'completely',\n",
       "       'meat.\"', '\"That\\'s', 'impossible.', 'What', 'about', 'the',\n",
       "       'radio', 'signals?', 'The', 'messages', 'to', 'the', 'stars?\"',\n",
       "       '\"They', 'use', 'the', 'radio', 'waves', 'to', 'talk,', 'but',\n",
       "       'the', 'signals', \"don't\", 'come', 'from', 'them.', 'The',\n",
       "       'signals', 'come', 'from', 'machines.\"', '\"So', 'who', 'made',\n",
       "       'the', 'machines?', \"That's\", 'who', 'we', 'want', 'to',\n",
       "       'contact.\"', '\"They', 'made', 'the', 'machines.', \"That's\", 'what',\n",
       "       \"I'm\", 'trying', 'to', 'tell', 'you.', 'Meat', 'made', 'the',\n",
       "       'machines.\"', '\"That\\'s', 'ridiculous.', 'How', 'can', 'meat',\n",
       "       'make', 'a', 'machine?', \"You're\", 'asking', 'me', 'to', 'believe',\n",
       "       'in', 'sentient', 'meat.\"', '\"I\\'m', 'not', 'asking', 'you,',\n",
       "       \"I'm\", 'telling', 'you.', 'These', 'creatures', 'are', 'the',\n",
       "       'only', 'sentient', 'race', 'in', 'that', 'sector', 'and',\n",
       "       \"they're\", 'made', 'out', 'of', 'meat.\"photomaxmix', '\"Maybe',\n",
       "       \"they're\", 'like', 'the', 'orfolei.', 'You', 'know,', 'a',\n",
       "       'carbon-based', 'intelligence', 'that', 'goes', 'through', 'a',\n",
       "       'meat', 'stage.\"', '\"Nope.', \"They're\", 'born', 'meat', 'and',\n",
       "       'they', 'die', 'meat.', 'We', 'studied', 'them', 'for', 'several',\n",
       "       'of', 'their', 'life', 'spans,', 'which', \"didn't\", 'take',\n",
       "       'long.', 'Do', 'you', 'have', 'any', 'idea', \"what's\", 'the',\n",
       "       'life', 'span', 'of', 'meat?\"', '\"Spare', 'me.', 'Okay,', 'maybe',\n",
       "       \"they're\", 'only', 'part', 'meat.', 'You', 'know,', 'like', 'the',\n",
       "       'weddilei.', 'A', 'meat', 'head', 'with', 'an', 'electron',\n",
       "       'plasma', 'brain', 'inside.\"', '\"Nope.', 'We', 'thought', 'of',\n",
       "       'that,', 'since', 'they', 'do', 'have', 'meat', 'heads,', 'like',\n",
       "       'the', 'weddilei.', 'But', 'I', 'told', 'you,', 'we', 'probed',\n",
       "       'them.', \"They're\", 'meat', 'all', 'the', 'way', 'through.\"',\n",
       "       '\"No', 'brain?\"', '\"Oh,', \"there's\", 'a', 'brain', 'all', 'right.',\n",
       "       \"It's\", 'just', 'that', 'the', 'brain', 'is', 'made', 'out', 'of',\n",
       "       'meat!', \"That's\", 'what', \"I've\", 'been', 'trying', 'to', 'tell',\n",
       "       'you.\"', '\"So', '...', 'what', 'does', 'the', 'thinking?\"',\n",
       "       '\"You\\'re', 'not', 'understanding,', 'are', 'you?', \"You're\",\n",
       "       'refusing', 'to', 'deal', 'with', 'what', \"I'm\", 'telling', 'you.',\n",
       "       'The', 'brain', 'does', 'the', 'thinking.', 'The', 'meat.\"',\n",
       "       '\"Thinking', 'meat!', \"You're\", 'asking', 'me', 'to', 'believe',\n",
       "       'in', 'thinking', 'meat!\"', '\"Yes,', 'thinking', 'meat!',\n",
       "       'Conscious', 'meat!', 'Loving', 'meat.', 'Dreaming', 'meat.',\n",
       "       'The', 'meat', 'is', 'the', 'whole', 'deal!', 'Are', 'you',\n",
       "       'beginning', 'to', 'get', 'the', 'picture', 'or', 'do', 'I',\n",
       "       'have', 'to', 'start', 'all', 'over?\"', '\"Omigod.', \"You're\",\n",
       "       'serious', 'then.', \"They're\", 'made', 'out', 'of', 'meat.\"',\n",
       "       '\"Thank', 'you.', 'Finally.', 'Yes.', 'They', 'are', 'indeed',\n",
       "       'made', 'out', 'of', 'meat.', 'And', \"they've\", 'been', 'trying',\n",
       "       'to', 'get', 'in', 'touch', 'with', 'us', 'for', 'almost', 'a',\n",
       "       'hundred', 'of', 'their', 'years.\"', '\"Omigod.', 'So', 'what',\n",
       "       'does', 'this', 'meat', 'have', 'in', 'mind?\"', '\"First', 'it',\n",
       "       'wants', 'to', 'talk', 'to', 'us.', 'Then', 'I', 'imagine', 'it',\n",
       "       'wants', 'to', 'explore', 'the', 'Universe,', 'contact', 'other',\n",
       "       'sentiences,', 'swap', 'ideas', 'and', 'information.', 'The',\n",
       "       'usual.\"', '\"We\\'re', 'supposed', 'to', 'talk', 'to', 'meat.\"',\n",
       "       '\"That\\'s', 'the', 'idea.', \"That's\", 'the', 'message', \"they're\",\n",
       "       'sending', 'out', 'by', 'radio.', \"'Hello.\", 'Anyone', 'out',\n",
       "       'there.', 'Anybody', \"home.'\", 'That', 'sort', 'of', 'thing.\"',\n",
       "       '\"They', 'actually', 'do', 'talk,', 'then.', 'They', 'use',\n",
       "       'words,', 'ideas,', 'concepts?\"', '\"Oh,', 'yes.', 'Except', 'they',\n",
       "       'do', 'it', 'with', 'meat.\"', '\"I', 'thought', 'you', 'just',\n",
       "       'told', 'me', 'they', 'used', 'radio.\"', '\"They', 'do,', 'but',\n",
       "       'what', 'do', 'you', 'think', 'is', 'on', 'the', 'radio?', 'Meat',\n",
       "       'sounds.', 'You', 'know', 'how', 'when', 'you', 'slap', 'or',\n",
       "       'flap', 'meat,', 'it', 'makes', 'a', 'noise?', 'They', 'talk',\n",
       "       'by', 'flapping', 'their', 'meat', 'at', 'each', 'other.', 'They',\n",
       "       'can', 'even', 'sing', 'by', 'squirting', 'air', 'through',\n",
       "       'their', 'meat.\"', '\"Omigod.', 'Singing', 'meat.', 'This', 'is',\n",
       "       'altogether', 'too', 'much.', 'So', 'what', 'do', 'you',\n",
       "       'advise?\"', '\"Officially', 'or', 'unofficially?\"', '\"Both.\"',\n",
       "       '\"Officially,', 'we', 'are', 'required', 'to', 'contact,',\n",
       "       'welcome', 'and', 'log', 'in', 'any', 'and', 'all', 'sentient',\n",
       "       'races', 'or', 'multibeings', 'in', 'this', 'quadrant', 'of',\n",
       "       'the', 'Universe,', 'without', 'prejudice,', 'fear', 'or',\n",
       "       'favor.', 'Unofficially,', 'I', 'advise', 'that', 'we', 'erase',\n",
       "       'the', 'records', 'and', 'forget', 'the', 'whole', 'thing.\"', '\"I',\n",
       "       'was', 'hoping', 'you', 'would', 'say', 'that.\"', '\"It', 'seems',\n",
       "       'harsh,', 'but', 'there', 'is', 'a', 'limit.', 'Do', 'we',\n",
       "       'really', 'want', 'to', 'make', 'contact', 'with', 'meat?\"', '\"I',\n",
       "       'agree', 'one', 'hundred', 'percent.', \"What's\", 'there', 'to',\n",
       "       'say?', \"'Hello,\", 'meat.', \"How's\", 'it', \"going?'\", 'But',\n",
       "       'will', 'this', 'work?', 'How', 'many', 'planets', 'are', 'we',\n",
       "       'dealing', 'with', 'here?\"', '\"Just', 'one.', 'They', 'can',\n",
       "       'travel', 'to', 'other', 'planets', 'in', 'special', 'meat',\n",
       "       'containers,', 'but', 'they', \"can't\", 'live', 'on', 'them.',\n",
       "       'And', 'being', 'meat,', 'they', 'can', 'only', 'travel',\n",
       "       'through', 'C', 'space.', 'Which', 'limits', 'them', 'to', 'the',\n",
       "       'speed', 'of', 'light', 'and', 'makes', 'the', 'possibility', 'of',\n",
       "       'their', 'ever', 'making', 'contact', 'pretty', 'slim.',\n",
       "       'Infinitesimal,', 'in', 'fact.\"', '\"So', 'we', 'just', 'pretend',\n",
       "       \"there's\", 'no', 'one', 'home', 'in', 'the', 'Universe.\"',\n",
       "       '\"That\\'s', 'it.\"', '\"Cruel.', 'But', 'you', 'said', 'it',\n",
       "       'yourself,', 'who', 'wants', 'to', 'meet', 'meat?', 'And', 'the',\n",
       "       'ones', 'who', 'have', 'been', 'aboard', 'our', 'vessels,', 'the',\n",
       "       'ones', 'you', 'probed?', \"You're\", 'sure', 'they', \"won't\",\n",
       "       'remember?\"', '\"They\\'ll', 'be', 'considered', 'crackpots', 'if',\n",
       "       'they', 'do.', 'We', 'went', 'into', 'their', 'heads', 'and',\n",
       "       'smoothed', 'out', 'their', 'meat', 'so', 'that', \"we're\", 'just',\n",
       "       'a', 'dream', 'to', 'them.\"', '\"A', 'dream', 'to', 'meat!', 'How',\n",
       "       'strangely', 'appropriate,', 'that', 'we', 'should', 'be',\n",
       "       \"meat's\", 'dream.\"', '\"And', 'we', 'marked', 'the', 'entire',\n",
       "       'sector', 'unoccupied.\"', '\"Good.', 'Agreed,', 'officially', 'and',\n",
       "       'unofficially.', 'Case', 'closed.', 'Any', 'others?', 'Anyone',\n",
       "       'interesting', 'on', 'that', 'side', 'of', 'the', 'galaxy?\"',\n",
       "       '\"Yes,', 'a', 'rather', 'shy', 'but', 'sweet', 'hydrogen', 'core',\n",
       "       'cluster', 'intelligence', 'in', 'a', 'class', 'nine', 'star',\n",
       "       'in', 'G445', 'zone.', 'Was', 'in', 'contact', 'two', 'galactic',\n",
       "       'rotations', 'ago,', 'wants', 'to', 'be', 'friendly', 'again.\"',\n",
       "       '\"They', 'always', 'come', 'around.\"', '\"And', 'why', 'not?',\n",
       "       'Imagine', 'how', 'unbearably,', 'how', 'unutterably', 'cold',\n",
       "       'the', 'Universe', 'would', 'be', 'if', 'one', 'were', 'all',\n",
       "       'alone', '...\"'], dtype='<U17')"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rmqipDBkDOTz"
   },
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6JTd7_0DPOx"
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "\n",
    "    # 2-layer LSTM, each layer has n_hidden units.\n",
    "    # Average Accuracy= 95.20% at 50k iter\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "\n",
    "    # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
    "    # Average Accuracy= 90.60% 50k iter\n",
    "    # Uncomment line below to test but comment out the 2-layer rnn.MultiRNNCell above\n",
    "    # rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xcXGPnZcu_f"
   },
   "outputs": [],
   "source": [
    "dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "vocab_size = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dwvvV-Y3MC-v"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.05\n",
    "training_iters = 50000\n",
    "display_step = 1000\n",
    "n_input = 3\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "# RNN output node weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "8nordcRf4Kmf",
    "outputId": "ad197e32-036d-4d57-c412-57b94edc43f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-db32b4dd1591>:12: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-10-db32b4dd1591>:12: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-10-db32b4dd1591>:20: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-13-376ef4242668>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "A2eqQd9l4Lza",
    "outputId": "6ddcc49f-e29b-4032-a657-89834be6a254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 1000, Average Loss= 20.605031, Average Accuracy= 1.20%\n",
      "['Any', 'others?', 'Anyone'] - [interesting] vs [closed.]\n",
      "Iter= 2000, Average Loss= 22.708942, Average Accuracy= 0.90%\n",
      "[\"You're\", 'sure', 'they'] - [won't] vs [probed?]\n",
      "Iter= 3000, Average Loss= 22.869608, Average Accuracy= 0.70%\n",
      "['them', 'to', 'the'] - [speed] vs [limits]\n",
      "Iter= 4000, Average Loss= 22.729935, Average Accuracy= 0.80%\n",
      "['want', 'to', 'make'] - [contact] vs [really]\n",
      "Iter= 5000, Average Loss= 22.505232, Average Accuracy= 0.80%\n",
      "['are', 'required', 'to'] - [contact,] vs [we]\n",
      "Iter= 6000, Average Loss= 22.572218, Average Accuracy= 0.80%\n",
      "['do', 'you', 'think'] - [is] vs [what]\n",
      "Iter= 7000, Average Loss= 22.387144, Average Accuracy= 0.80%\n",
      "['to', 'talk', 'to'] - [meat.\"] vs [supposed]\n",
      "Iter= 8000, Average Loss= 22.285392, Average Accuracy= 0.70%\n",
      "['Finally.', 'Yes.', 'They'] - [are] vs [you.]\n",
      "Iter= 9000, Average Loss= 22.271237, Average Accuracy= 0.80%\n",
      "['telling', 'you.', 'The'] - [brain] vs [I'm]\n",
      "Iter= 10000, Average Loss= 22.165112, Average Accuracy= 0.80%\n",
      "['told', 'you,', 'we'] - [probed] vs [I]\n",
      "Iter= 11000, Average Loss= 22.200960, Average Accuracy= 0.70%\n",
      "['for', 'several', 'of'] - [their] vs [them]\n",
      "Iter= 12000, Average Loss= 22.163227, Average Accuracy= 0.90%\n",
      "[\"You're\", 'asking', 'me'] - [to] vs [machine?]\n",
      "Iter= 13000, Average Loss= 22.101531, Average Accuracy= 0.60%\n",
      "['The', 'messages', 'to'] - [the] vs [signals?]\n",
      "Iter= 14000, Average Loss= 22.137492, Average Accuracy= 1.20%\n",
      "['how', 'unbearably,', 'how'] - [unutterably] vs [Imagine]\n",
      "Iter= 15000, Average Loss= 22.045487, Average Accuracy= 1.00%\n",
      "['marked', 'the', 'entire'] - [sector] vs [we]\n",
      "Iter= 16000, Average Loss= 22.117432, Average Accuracy= 0.70%\n",
      "['who', 'have', 'been'] - [aboard] vs [ones]\n",
      "Iter= 17000, Average Loss= 21.971440, Average Accuracy= 0.70%\n",
      "['being', 'meat,', 'they'] - [can] vs [And]\n",
      "Iter= 18000, Average Loss= 22.044660, Average Accuracy= 0.70%\n",
      "['seems', 'harsh,', 'but'] - [there] vs [\"It]\n",
      "Iter= 19000, Average Loss= 21.931470, Average Accuracy= 0.60%\n",
      "['is', 'altogether', 'too'] - [much.] vs [This]\n",
      "Iter= 20000, Average Loss= 21.942002, Average Accuracy= 0.70%\n",
      "['\"I', 'thought', 'you'] - [just] vs [meat.\"]\n",
      "Iter= 21000, Average Loss= 21.928139, Average Accuracy= 0.70%\n",
      "['to', 'explore', 'the'] - [Universe,] vs [wants]\n",
      "Iter= 22000, Average Loss= 21.922241, Average Accuracy= 0.80%\n",
      "['out', 'of', 'meat.\"'] - [\"Thank] vs [made]\n",
      "Iter= 23000, Average Loss= 22.275198, Average Accuracy= 1.00%\n",
      "['deal', 'with', 'what'] - [I'm] vs [to]\n",
      "Iter= 24000, Average Loss= 22.002035, Average Accuracy= 0.80%\n",
      "['heads,', 'like', 'the'] - [weddilei.] vs [meat]\n",
      "Iter= 25000, Average Loss= 22.042321, Average Accuracy= 0.90%\n",
      "['several', 'of', 'their'] - [life] vs [for]\n",
      "Iter= 26000, Average Loss= 22.020609, Average Accuracy= 0.70%\n",
      "[\"You're\", 'asking', 'me'] - [to] vs [machine?]\n",
      "Iter= 27000, Average Loss= 22.022922, Average Accuracy= 0.80%\n",
      "['messages', 'to', 'the'] - [stars?\"] vs [The]\n",
      "Iter= 28000, Average Loss= 22.038023, Average Accuracy= 0.90%\n",
      "['were', 'all', 'alone'] - [...\"] vs [one]\n",
      "Iter= 29000, Average Loss= 21.967235, Average Accuracy= 0.80%\n",
      "['unofficially.', 'Case', 'closed.'] - [Any] vs [and]\n",
      "Iter= 30000, Average Loss= 21.996544, Average Accuracy= 0.70%\n",
      "[\"You're\", 'sure', 'they'] - [won't] vs [probed?]\n",
      "Iter= 31000, Average Loss= 21.996205, Average Accuracy= 0.90%\n",
      "['Which', 'limits', 'them'] - [to] vs [space.]\n",
      "Iter= 32000, Average Loss= 22.061326, Average Accuracy= 0.80%\n",
      "['one', 'hundred', 'percent.'] - [What's] vs [agree]\n",
      "Iter= 33000, Average Loss= 22.004585, Average Accuracy= 0.70%\n",
      "['any', 'and', 'all'] - [sentient] vs [in]\n",
      "Iter= 34000, Average Loss= 21.981833, Average Accuracy= 0.80%\n",
      "['sounds.', 'You', 'know'] - [how] vs [Meat]\n",
      "Iter= 35000, Average Loss= 22.002186, Average Accuracy= 0.80%\n",
      "['idea.', \"That's\", 'the'] - [message] vs [the]\n",
      "Iter= 36000, Average Loss= 21.999417, Average Accuracy= 0.80%\n",
      "['touch', 'with', 'us'] - [for] vs [in]\n",
      "Iter= 37000, Average Loss= 22.049112, Average Accuracy= 0.90%\n",
      "['meat!', 'Loving', 'meat.'] - [Dreaming] vs [Conscious]\n",
      "Iter= 38000, Average Loss= 22.155234, Average Accuracy= 0.90%\n",
      "['brain', 'is', 'made'] - [out] vs [the]\n",
      "Iter= 39000, Average Loss= 21.985493, Average Accuracy= 0.60%\n",
      "['only', 'part', 'meat.'] - [You] vs [they're]\n",
      "Iter= 40000, Average Loss= 22.022578, Average Accuracy= 0.80%\n",
      "['sector', 'and', \"they're\"] - [made] vs [that]\n",
      "Iter= 41000, Average Loss= 22.042548, Average Accuracy= 0.90%\n",
      "['who', 'made', 'the'] - [machines?] vs [\"So]\n",
      "Iter= 42000, Average Loss= 22.035658, Average Accuracy= 1.00%\n",
      "['planet,', 'took', 'them'] - [aboard] vs [the]\n",
      "Iter= 43000, Average Loss= 22.105703, Average Accuracy= 0.70%\n",
      "['zone.', 'Was', 'in'] - [contact] vs [G445]\n",
      "Iter= 44000, Average Loss= 21.985504, Average Accuracy= 0.80%\n",
      "['meat', 'so', 'that'] - [we're] vs [their]\n",
      "Iter= 45000, Average Loss= 21.973070, Average Accuracy= 0.70%\n",
      "['in', 'fact.\"', '\"So'] - [we] vs [Infinitesimal,]\n",
      "Iter= 46000, Average Loss= 21.839638, Average Accuracy= 1.00%\n",
      "['will', 'this', 'work?'] - [How] vs [But]\n",
      "Iter= 47000, Average Loss= 21.865535, Average Accuracy= 0.70%\n",
      "['fear', 'or', 'favor.'] - [Unofficially,] vs [prejudice,]\n",
      "Iter= 48000, Average Loss= 21.985792, Average Accuracy= 0.60%\n",
      "['flapping', 'their', 'meat'] - [at] vs [by]\n",
      "Iter= 49000, Average Loss= 21.706695, Average Accuracy= 0.70%\n",
      "['do', 'talk,', 'then.'] - [They] vs [actually]\n",
      "Iter= 50000, Average Loss= 21.621158, Average Accuracy= 0.70%\n",
      "['this', 'meat', 'have'] - [in] vs [does]\n",
      "Optimization Finished!\n",
      "Elapsed time:  4.978296899795533 min\n",
      "3 words: who are you\n",
      "who are you in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c947d047512d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s words: \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    while step < training_iters:\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = training_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        prompt = \"%s words: \" % n_input\n",
    "        sentence = input(prompt)\n",
    "        sentence = sentence.strip()\n",
    "        words = sentence.split(' ')\n",
    "        if len(words) != n_input:\n",
    "            continue\n",
    "        try:\n",
    "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "            for i in range(32):\n",
    "                keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "                onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "                symbols_in_keys = symbols_in_keys[1:]\n",
    "                symbols_in_keys.append(onehot_pred_index)\n",
    "            print(sentence)\n",
    "        except:\n",
    "            print(\"Word not in dictionary\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
