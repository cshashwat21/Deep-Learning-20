{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JG-D3eEpDehQ"
   },
   "source": [
    "## Import Data and Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBL8Nnn3DehS"
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-1kHv6VDehY"
   },
   "source": [
    "## Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1581170244339,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "E7s7tQjxDehZ",
    "outputId": "811accd8-5bd9-4c5a-f4e0-1beba50b07b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascii_letters :  abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      "ascii_lowercase :  abcdefghijklmnopqrstuvwxyz\n",
      "ascii_uppercase :  ABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      "digits :  0123456789\n",
      "hexdigits :  0123456789abcdefABCDEF\n",
      "whitespace :   \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "punctuation :  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print('ascii_letters : ', string.ascii_letters)\n",
    "print('ascii_lowercase : ', string.ascii_lowercase)\n",
    "print('ascii_uppercase : ', string.ascii_uppercase)\n",
    "print('digits : ', string.digits)\n",
    "print('hexdigits : ',string.hexdigits)\n",
    "print('whitespace : ', string.whitespace)  # ' \\t\\n\\r\\x0b\\x0c'\n",
    "print('punctuation : ', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFHPe-3nDehe"
   },
   "source": [
    "#### All functions and attribute of string module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 904,
     "status": "ok",
     "timestamp": 1581170247516,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "eWSOhiToDehf",
    "outputId": "cc8a210b-486f-4e3a-e966-ca7781455ebd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Formatter',\n",
       " 'Template',\n",
       " '_ChainMap',\n",
       " '_TemplateMetaclass',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_re',\n",
       " '_string',\n",
       " 'ascii_letters',\n",
       " 'ascii_lowercase',\n",
       " 'ascii_uppercase',\n",
       " 'capwords',\n",
       " 'digits',\n",
       " 'hexdigits',\n",
       " 'octdigits',\n",
       " 'printable',\n",
       " 'punctuation',\n",
       " 'whitespace']"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "suUFUFzHDehj"
   },
   "source": [
    "## String cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1011,
     "status": "ok",
     "timestamp": 1581170250111,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "IS2Jmo_KDehk",
    "outputId": "ba4a5142-e263-47ba-e695-08ae60c6e16b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'o', 'r', ' ', 'm', 'e', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'v', 'e', ' ', 's', 'h', 'o', 'u', 'l', 'd', ' ', 's', 't', 'a', 'r', 't', ' ', 'w', 'i', 't', 'h', ' ', 'a', 't', 't', 'r', 'a', 'c', 't', 'i', 'o', 'n', '!', '.', '.', '.', ' ']\n"
     ]
    }
   ],
   "source": [
    "random_str = 'For me the love should start with attraction!... '\n",
    "random_str_char = [char for char in random_str]\n",
    "print(random_str_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iXhyBYzbDehq"
   },
   "source": [
    "#### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 928,
     "status": "ok",
     "timestamp": 1581170254201,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "Ys2cA50kDehr",
    "outputId": "199f6c25-074f-417c-a4db-54cf3d67c1a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'o', 'r', ' ', 'm', 'e', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'v', 'e', ' ', 's', 'h', 'o', 'u', 'l', 'd', ' ', 's', 't', 'a', 'r', 't', ' ', 'w', 'i', 't', 'h', ' ', 'a', 't', 't', 'r', 'a', 'c', 't', 'i', 'o', 'n', ' ']\n"
     ]
    }
   ],
   "source": [
    "non_punc_str = [char for char in random_str if char not in string.punctuation]\n",
    "print(non_punc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 880,
     "status": "ok",
     "timestamp": 1581170256219,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "PC8e6v-YDehw",
    "outputId": "cc93aae4-e816-4e12-a86c-2ddbaff96093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete string without Punctuation :  For me the love should start with attraction \n"
     ]
    }
   ],
   "source": [
    "non_punc_str = ''.join(non_punc_str)\n",
    "print(\"Complete string without Punctuation : \", non_punc_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dho5w80iDeh0"
   },
   "source": [
    "#### Remove Special Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 977,
     "status": "ok",
     "timestamp": 1581170260452,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "PjtpYweIDeh1",
    "outputId": "39b00d7e-5cf4-4df2-9a78-f7c187d7edc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'o', 'r', ' ', 'm', 'e', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'v', 'e', ' ', 's', 'h', 'o', 'u', 'l', 'd', ' ', 's', 't', 'a', 'r', 't', ' ', 'w', 'i', 't', 'h', ' ', 'a', 't', 't', 'r', 'a', 'c', 't', 'i', 'o', 'n', '!', '.', '.', '.', ' ', ' ', '~', '!', '@', '#', ' ', '$', '%', '^', '&', '*', '(', ')', '_', '_', '+', '=', '{', '|', ':', '\"', ' ', '<', '>', '?', ' ', '4', '3', '5', '6', '7', '8', '9', '6', '5', '4', ' ', '$', '%', '^', '&', '*', '(', '&', '^', '%', ' ', ')', '\"', ' ', '}']\n"
     ]
    }
   ],
   "source": [
    "special_char_str = 'For me the love should start with attraction!...  ~!@# $%^&*()__+={|:\" <>? 4356789654 $%^&*(&^% )\" }'\n",
    "special_char = [char for char in special_char_str]\n",
    "print(special_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1581170263798,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "uoH1GmG3Deh6",
    "outputId": "9e5c1c2c-d5b8-49b0-9685-29058a413603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'o', 'r', ' ', 'm', 'e', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'v', 'e', ' ', 's', 'h', 'o', 'u', 'l', 'd', ' ', 's', 't', 'a', 'r', 't', ' ', 'w', 'i', 't', 'h', ' ', 'a', 't', 't', 'r', 'a', 'c', 't', 'i', 'o', 'n', ' ', ' ', ' ', ' ', ' ', '4', '3', '5', '6', '7', '8', '9', '6', '5', '4', ' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "non_punc_special_char_str = [char for char in special_char_str if char not in string.punctuation]\n",
    "print(non_punc_special_char_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1581170292575,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "hkE5-HUNDeh_",
    "outputId": "7dac537c-deae-414d-c14d-7d6ef00c1941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete string without  Special Character :  For me the love should start with attraction     4356789654   \n"
     ]
    }
   ],
   "source": [
    "non_punc_special_char_str = ''.join(non_punc_special_char_str)\n",
    "print(\"Complete string without  Special Character : \", non_punc_special_char_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vz6HGx0VDeiF"
   },
   "source": [
    "####  Stopwords\n",
    "\n",
    "These are common words that don’t really add anything to the classification, such as a, able, either, else, ever and so on. \n",
    "\n",
    "**For Ex :** \n",
    "1. The election was over would be : election over.\n",
    "2. A very close game would be : very close game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eClqaPAiDeiG"
   },
   "source": [
    "#### Tokenization of words\n",
    "We use the method **word_tokenize** to split a sentence into words. The output of word tokenization can be converted to Data Frame for better text understanding in machine learning applications.\n",
    "\n",
    "Link : https://pythonprogramming.net/stop-words-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZ1e-B89DeiI"
   },
   "source": [
    "#### Tokenizing of sentences\n",
    "The same principle can be applied to sentences. Simply change the to **sent_tokenize**\n",
    "We have added two sentences to the variable data.\n",
    "\n",
    "Link : https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1909,
     "status": "ok",
     "timestamp": 1581170501900,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "1mbTB8ODGJ6z",
    "outputId": "5c8c1a85-bc5e-4f37-f764-79a4a05d4a31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OBt3YXkrDeiJ"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1581170443405,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "WMCwu0-rDeiM",
    "outputId": "f2e43671-d5f8-4a3d-d36d-f2d0660a8055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Stopwords :  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords.words('english')[0:10]\n",
    "en_stopwords = stopwords.words('english')\n",
    "print(\"English Stopwords : \", en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1581170458200,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "o-b8JcBTDeiQ",
    "outputId": "933f29aa-1ed3-4671-8db6-cf2e966a00a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokens :  ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "filtered_sentence :  ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(\"word_tokens : \", word_tokens)\n",
    "print(\"filtered_sentence : \", filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1039,
     "status": "ok",
     "timestamp": 1581170473520,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "dzMCqd6ODeiU",
    "outputId": "6ffdf318-2a73-4d14-bd50-ffc48f6fec2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All work and no play makes jack dull boy.', 'All work and no play makes jack a dull boy.']\n"
     ]
    }
   ],
   "source": [
    "example_sent_data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "print(sent_tokenize(example_sent_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pR6V9ZvXDeiY"
   },
   "source": [
    "#### Lemmatizing words\n",
    "\n",
    "This is grouping together different inflections of the same word. So election, elections, elected, and so on would be grouped together and counted as more appearances of the same word.\n",
    "\n",
    "Link : https://pythonprogramming.net/lemmatizing-nltk-tutorial/\n",
    "\n",
    "\n",
    "Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning. Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as the lemma. The NLTK Lemmatization method is based on WorldNet's built-in morph function. Text preprocessing includes both stemming as well as lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w5ThlFAeDeiZ"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1581170510675,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "mMIwMkGLDeig",
    "outputId": "1efde0be-7c51-48c6-bf37-26133508f055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_words :  ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "sentence_words = word_tokenize(sentence)\n",
    "print(\"sentence_words : \", sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2555,
     "status": "ok",
     "timestamp": 1581170515429,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "yyFhCw0nDeik",
    "outputId": "bb5ae4fa-0716-40e7-8839-0404e938cb87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "The                 The                 \n",
      "striped             striped             \n",
      "bats                bat                 \n",
      "are                 are                 \n",
      "hanging             hanging             \n",
      "on                  on                  \n",
      "their               their               \n",
      "feet                foot                \n",
      "for                 for                 \n",
      "best                best                \n"
     ]
    }
   ],
   "source": [
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDkYVMf3Dein"
   },
   "source": [
    "In the above output, you must be wondering that no actual **root form** has been given for any word, this is because they are given without context. **You need to provide the context in which you want to lemmatize that is the parts-of-speech (POS)**. This is done by giving the value for pos parameter in **wordnet_lemmatizer.lemmatize**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 887,
     "status": "ok",
     "timestamp": 1581170547401,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "YI9SpMaMDeio",
    "outputId": "cbb7f833-49fc-47ac-d2dd-4150c87fd1ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The                 The                 \n",
      "striped             strip               \n",
      "bats                bat                 \n",
      "are                 be                  \n",
      "hanging             hang                \n",
      "on                  on                  \n",
      "their               their               \n",
      "feet                feet                \n",
      "for                 for                 \n",
      "best                best                \n"
     ]
    }
   ],
   "source": [
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\"))) # pos=\"v\" : verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 949,
     "status": "ok",
     "timestamp": 1581170600602,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "OjslU0EnDeiu",
    "outputId": "7fb24525-4a12-4e70-8b32-002653ad0a0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "election :  election\n",
      "elections :  election\n",
      "elected :  elected\n",
      "rocks :  rock\n",
      "python :  python\n",
      "better :  better\n",
      "better - a:  good\n",
      "best :  best\n",
      "best - a :  best\n",
      "ran :  ran\n",
      "ran - v :  run\n",
      "caring :  caring\n",
      "caring - v :  care\n"
     ]
    }
   ],
   "source": [
    "print(\"election : \", wordnet_lemmatizer.lemmatize(\"election\"))\n",
    "print(\"elections : \", wordnet_lemmatizer.lemmatize(\"elections\"))\n",
    "print(\"elected : \", wordnet_lemmatizer.lemmatize(\"elected\"))\n",
    "print(\"rocks : \", wordnet_lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"python : \", wordnet_lemmatizer.lemmatize(\"python\"))\n",
    "print(\"better : \", wordnet_lemmatizer.lemmatize(\"better\"))\n",
    "print(\"better - a: \", wordnet_lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(\"best : \", wordnet_lemmatizer.lemmatize(\"best\"))\n",
    "print(\"best - a : \", wordnet_lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(\"ran : \",  wordnet_lemmatizer.lemmatize(\"ran\"))\n",
    "print(\"ran - v : \",  wordnet_lemmatizer.lemmatize(\"ran\",'v'))\n",
    "print(\"caring : \",  wordnet_lemmatizer.lemmatize(\"caring\"))\n",
    "print(\"caring - v : \",  wordnet_lemmatizer.lemmatize(\"caring\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PldTSpN_Deiy"
   },
   "source": [
    "#### Lemmatizer with POS Tag\n",
    "\n",
    "It may not be possible manually provide the corrent POS tag for every word for large texts. So, instead, we will find out the correct **POS** tag for each word, map it to the right input character that the WordnetLemmatizer accepts and pass it as the second argument to **lemmatize**.\n",
    "\n",
    "**So how to get the POS tag for a given word?**\n",
    "\n",
    "In nltk, it is available through the **nltk.pos_tag()** method. It accepts only a list (list of words), even if its a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 997,
     "status": "ok",
     "timestamp": 1581170626039,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "spSaC3RqDeiy",
    "outputId": "eab96eee-7112-4e31-e342-426be443ccfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[('feet', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "word_pos_tag = nltk.pos_tag(['feet'])\n",
    "print(word_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 991,
     "status": "ok",
     "timestamp": 1581170671680,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "KbqsRhz3Dei2",
    "outputId": "6f419895-db15-42d9-856a-4bb67b3f8277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('striped', 'JJ'), ('bats', 'NNS'), ('are', 'VBP'), ('hanging', 'VBG'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('for', 'IN'), ('best', 'JJS')]\n"
     ]
    }
   ],
   "source": [
    "sentence_pos_tag = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "print(sentence_pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vwg7Q2WgDejG"
   },
   "source": [
    "**Note**\n",
    "\n",
    "**nltk.pos_tag()** returns a tuple with the POS tag. The key here is, how to **lemmatizing sentences** ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 944,
     "status": "ok",
     "timestamp": 1581170685114,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "ArxA9c5yDejH",
    "outputId": "3fd95124-2124-403a-bfa2-1dd2776daf6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am loving it\n",
      "loving\n",
      "love\n",
      "I be love it\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "print(lemmatizer.lemmatize(\"I am loving it\"))\n",
    "print(lemmatizer.lemmatize(\"loving\"))\n",
    "print(lemmatizer.lemmatize(\"loving\", \"v\"))\n",
    "print(lemmatize_sentence(\"I am loving it\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kcsgUZZ0DejK"
   },
   "source": [
    "#### Stemming words\n",
    "\n",
    "Stemming is a kind of normalization for words. Normalization is a technique where a set of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized.\n",
    "\n",
    "In another word, there is one root word, but there are many variations of the same words. \n",
    "**For example**, the root word is \"eat\" and it's variations are \"eats, eating, eaten and like so\". In the same way, with the help of Stemming, we can find the root word of any variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1094,
     "status": "ok",
     "timestamp": 1581170759209,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "mmKAabOkDejM",
    "outputId": "dc5a63f3-813a-4a18-9b16-c8b521a5d9f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps = PorterStemmer()\n",
    "for w in e_words:\n",
    "    root_word = ps.stem(w)\n",
    "    print(root_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1581170764038,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "9sa_E7TJDejP",
    "outputId": "cf118c08-161b-4a8c-a936-c90a9310df7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "guru99\n",
      ",\n",
      "you\n",
      "have\n",
      "to\n",
      "build\n",
      "a\n",
      "veri\n",
      "good\n",
      "site\n",
      "and\n",
      "I\n",
      "love\n",
      "visit\n",
      "your\n",
      "site\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "stem_sentence = \"Hello Guru99, You have to build a very good site and I love visiting your site.\"\n",
    "words = word_tokenize(stem_sentence)\n",
    "ps = PorterStemmer()\n",
    "for w in words:\n",
    "    root_word = ps.stem(w)\n",
    "    print(root_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1nPmtwwDejS"
   },
   "source": [
    "#### Why is Lemmatization better than Stemming?\n",
    "\n",
    "Stemming algorithm works by cutting the suffix from the word. In a broader sense cuts either the beginning or end of the word.\n",
    "\n",
    "On the contrary, Lemmatization is a more powerful operation, and it takes into consideration morphological analysis of the words. It returns the lemma which is the base form of all its inflectional forms. In-depth linguistic knowledge is required to create dictionaries and look for the proper form of the word. Stemming is a general operation while lemmatization is an intelligent operation where the proper form will be looked in the dictionary. Hence, lemmatization helps in forming better machine learning features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7vu2-eqjDejT"
   },
   "source": [
    "#### Code to distinguish between Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4rYRXXFRDejV"
   },
   "source": [
    "#### Stemming code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 977,
     "status": "ok",
     "timestamp": 1581170878595,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "ypohdTV6DejW",
    "outputId": "32d204ab-f58d-4f68-d96c-91dad09c9e28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for studies is studi\n",
      "Stemming for studying is studi\n",
      "Stemming for cries is cri\n",
      "Stemming for cry is cri\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer  = PorterStemmer()\n",
    "text = \"studies studying cries cry\"\n",
    "\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZYgaCCfDeje"
   },
   "source": [
    "#### Lemmatization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1581170788395,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "japbYHN2Dejf",
    "outputId": "11219c94-621c-4c66-e53e-b35cfb4c2a7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-xgookaDejj"
   },
   "source": [
    "#### N-grams\n",
    "\n",
    "N-grams are contiguous sequences of n-items in a sentence. N can be 1, 2 or any other positive integers, although usually we do not consider very large N because those n-grams rarely appears in many different places.\n",
    "\n",
    "When performing machine learning tasks related to natural language processing, we usually need to generate n-grams from input sentences. For example, in text classification tasks, in addition to using each individual token found in the corpus, we may want to add bi-grams or tri-grams as features to represent our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 991,
     "status": "ok",
     "timestamp": 1581170802587,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "hBFk71fVDejk",
    "outputId": "cf5bdd3b-df1a-4ce0-97d3-4e1c2f502b78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a', 'foo', 'bar', 'sentences')\n",
      "('is', 'a', 'foo', 'bar', 'sentences', 'and')\n",
      "('a', 'foo', 'bar', 'sentences', 'and', 'i')\n",
      "('foo', 'bar', 'sentences', 'and', 'i', 'want')\n",
      "('bar', 'sentences', 'and', 'i', 'want', 'to')\n",
      "('sentences', 'and', 'i', 'want', 'to', 'ngramize')\n",
      "('and', 'i', 'want', 'to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "ng_sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "\n",
    "n_gram = 6\n",
    "sixgrams = ngrams(ng_sentence.split(), n_gram)\n",
    "for grams in sixgrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QGUcVnbDejo"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def get_ngrams(text, n ):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1581170812140,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "ewAV0mMSDejs",
    "outputId": "bd882905-5a25-439d-b7c6-549ce844d89c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the',\n",
       " 'is the simplest',\n",
       " 'the simplest text',\n",
       " 'simplest text i',\n",
       " 'text i could',\n",
       " 'i could think',\n",
       " 'could think of']"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ngrams('This is the simplest text i could think of', 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ho84amagDejv"
   },
   "source": [
    "#### Word Counts with CountVectorizer\n",
    "\n",
    "The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n",
    "\n",
    "Link : https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1125,
     "status": "ok",
     "timestamp": 1581171476896,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "cDVuapQEDejv",
    "outputId": "14ce5fb7-76ac-40b5-f683-0bbb60ed0312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_ :  {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "Shpae :  (1, 8)\n",
      "O/P vector (bag-of-words) :  [[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#List of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "\n",
    "# Create the transform\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "#Tokenize and build vocab : Assign a unique number to each word as: also known as \"Tokenize\"\n",
    "#In ML terms: Learn a vocabulary dictionary of all tokens in the raw documents, and it is done by using CountVectorizer.fit()\n",
    "count_vectorizer.fit(text)\n",
    "\n",
    "# Summarize\n",
    "print(\"vocabulary_ : \", count_vectorizer.vocabulary_)\n",
    "\n",
    "# Encode document : Count the occurence of each word: basically in ML terms \"encoding documents\"\n",
    "# It is done by using CountVectorizer.transform()\n",
    "count_vector = count_vectorizer.transform(text)\n",
    "\n",
    "#Summarize encoded vector\n",
    "print(\"Shpae : \", count_vector.shape)\n",
    "print(\"O/P vector (bag-of-words) : \", count_vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJKVg9b0Dejy"
   },
   "source": [
    "#### Word Frequencies with TfidfVectorizer\n",
    "\n",
    "Word counts are a good starting point, but are very basic.\n",
    "\n",
    "One issue with simple counts is that some words like “the” will appear many times and their large counts will not be very meaningful in the encoded vectors.\n",
    "\n",
    "An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for “Term Frequency – Inverse Document” Frequency which are the components of the resulting scores assigned to each word.\n",
    "\n",
    "**Term Frequency:** This summarizes how often a given word appears within a document.\n",
    "\n",
    "**Inverse Document Frequency:** This downscales words that appear a lot across documents.\n",
    "\n",
    "The **TfidfVectorizer** will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Alternately, if you already have a learned **CountVectorizer**, you can use it with a **TfidfTransformer** to just calculate the inverse document frequencies and start encoding documents.\n",
    "\n",
    "The **scores are normalized to values between 0 and 1** and the encoded document vectors can then be used directly with most machine learning algorithms.\n",
    "\n",
    "Link : https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2303,
     "status": "ok",
     "timestamp": 1581177624642,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "sRStGO1-Dejz",
    "outputId": "504a5960-7cf6-4943-f3dd-6213425cdab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_ :  {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "idf_ :  [1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "shape :  (1, 8)\n",
      "O/P vector (bag-of-words) :  [[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "\n",
    "# create the transform\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "tfidf_vectorizer.fit(text)\n",
    "\n",
    "# Summarize\n",
    "print(\"vocabulary_ : \", tfidf_vectorizer.vocabulary_)\n",
    "print(\"idf_ : \", tfidf_vectorizer.idf_)\n",
    "\n",
    "# Encode document\n",
    "tfidf_vector = tfidf_vectorizer.transform([text[0]])\n",
    "\n",
    "#Summarize encoded vector\n",
    "print(\"shape : \", tfidf_vector.shape)\n",
    "print(\"O/P vector (bag-of-words) : \", tfidf_vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zsYdaCGyDej3"
   },
   "source": [
    "## CountVectorizer and TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VjiIStPQDej5"
   },
   "source": [
    "We can get the vocab and vectors using **CountVectorizer**,  then use **TfidfTransformer** to just calculate the inverse document frequencies and start encoding documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1989,
     "status": "ok",
     "timestamp": 1581177631461,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "llKIW-UlDej7",
    "outputId": "4f3165f6-8c6f-426f-b440-01360368dfde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Document Frequency (IDF) Vector :  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  (0, 7)\t0.6030226891555273\n",
      "  (0, 6)\t0.30151134457776363\n",
      "  (0, 5)\t0.30151134457776363\n",
      "  (0, 4)\t0.30151134457776363\n",
      "  (0, 3)\t0.30151134457776363\n",
      "  (0, 2)\t0.30151134457776363\n",
      "  (0, 1)\t0.30151134457776363\n",
      "  (0, 0)\t0.30151134457776363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "messages_tfidf_fit = tfidf_transformer.fit(count_vector)\n",
    "messages_tfidf_trans = tfidf_transformer.transform(count_vector)\n",
    "\n",
    "print(\"Inverse Document Frequency (IDF) Vector : \", messages_tfidf_fit.idf_)\n",
    "print(messages_tfidf_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3uenxftDej_"
   },
   "source": [
    "## Only TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6jAAGOCDekA"
   },
   "source": [
    "The **TfidfVectorizer** will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1131,
     "status": "ok",
     "timestamp": 1581177725288,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "lleSV-7-DekA",
    "outputId": "7e132f4b-d003-4968-b38e-7fe8c944d574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Document Frequency (IDF) Vector :  [1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Inverse Document Frequency (IDF) Vector : \", tfidf_vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1581177729968,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "G7cPHlHBDekD",
    "outputId": "79defc9d-cc53-4b48-cc8e-5dd22bef453d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.4298344050159891\n",
      "  (0, 6)\t0.3638864554802418\n",
      "  (0, 5)\t0.3638864554802418\n",
      "  (0, 4)\t0.3638864554802418\n",
      "  (0, 3)\t0.3638864554802418\n",
      "  (0, 2)\t0.27674502873103346\n",
      "  (0, 1)\t0.27674502873103346\n",
      "  (0, 0)\t0.3638864554802418\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXFnbU09DekG"
   },
   "source": [
    "#### Hashing with HashingVectorizer\n",
    "\n",
    "Counts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large.\n",
    "\n",
    "This, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms.\n",
    "\n",
    "A clever work around is to use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks).\n",
    "\n",
    "The HashingVectorizer class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1581177765499,
     "user": {
      "displayName": "Naveen Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDpB-kUtRt6uWF616crcRaK8gO7ErTMJtMxRbk0yg=s64",
      "userId": "03560093979069921128"
     },
     "user_tz": -330
    },
    "id": "euV00v-yDekI",
    "outputId": "8dd61d63-ef08-4d5a-a1db-5265a1fd89fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector :    (0, 5)\t0.3333333333333333\n",
      "  (0, 7)\t-0.3333333333333333\n",
      "  (0, 8)\t0.3333333333333333\n",
      "  (0, 11)\t0.3333333333333333\n",
      "  (0, 13)\t0.0\n",
      "  (0, 15)\t-0.3333333333333333\n",
      "  (0, 18)\t-0.6666666666666666\n",
      "shape :  (1, 20)\n",
      "O/P :  [[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "\n",
    "# Encode document\n",
    "vector = vectorizer.transform(text)\n",
    "print(\"vector : \", vector)\n",
    "\n",
    "# Summarize encoded vector\n",
    "print(\"shape : \", vector.shape)\n",
    "print(\"O/P : \", vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJpvO4QVidIx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vz6HGx0VDeiF",
    "eClqaPAiDeiG",
    "l1nPmtwwDejS",
    "7vu2-eqjDejT"
   ],
   "name": "Text Classification Pre-Processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
